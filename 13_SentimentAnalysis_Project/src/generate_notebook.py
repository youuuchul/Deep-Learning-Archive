import json
import os

notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 쇼핑몰 리뷰 감성 분석: Full Fine-Tuning vs PEFT (LoRA)\n",
    "\n",
    "## 1. 프로젝트 개요\n",
    "- **목표**: 패션 도메인 리뷰의 감성(긍정/중립/부정)을 분석하는 모델 최적화.\n",
    "- **Base Model**: `klue/roberta-base`\n",
    "- **비교 대상**: Full Fine-Tuning (전체 학습) vs LoRA (효율적 학습)\n",
    "- **환경**: Local (M1 Debugging) / Colab (Full Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 설치 (Colab 확인)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path if not present\n",
    "# This allows importing from 'src' when running in 'notebooks/'\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "    print(f\"Added project root to sys.path: {project_root}\")\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -q transformers peft datasets evaluate accelerate\n",
    "    print(\"Dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 유틸리티 및 데이터 로더 (Self-Contained)\n",
    "데이터 누수 방지를 위한 Group Split 로직이 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification, \n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n",
    "import evaluate\n",
    "\n",
    "# Logging Setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Device Detection\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "is_mac_local = (device.type == 'mps')\n",
    "is_colab = ('google.colab' in sys.modules)\n",
    "\n",
    "if is_mac_local:\n",
    "    logger.warning(\"Running on Mac M1 (MPS). Training will be limited to debugging samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader Logic\n",
    "label_mapping = {\"-1\": 0, \"0\": 1, \"1\": 2}\n",
    "\n",
    "def load_and_split_data(data_dir, sample_limit=None):\n",
    "    files = list(Path(data_dir).glob(\"**/*.json\"))\n",
    "    data = []\n",
    "    \n",
    "    logger.info(f\"Found {len(files)} files.\")\n",
    "    \n",
    "    for fpath in files:\n",
    "        try:\n",
    "            with open(fpath, encoding='utf-8') as f:\n",
    "                content = json.load(f)\n",
    "                items = content if isinstance(content, list) else []\n",
    "                for item in items:\n",
    "                    if item.get(\"RawText\") and item.get(\"GeneralPolarity\") in label_mapping:\n",
    "                        data.append({\n",
    "                            \"text\": item.get(\"RawText\"),\n",
    "                            \"label\": label_mapping[item.get(\"GeneralPolarity\")],\n",
    "                            \"product_name\": item.get(\"ProductName\")\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "            \n",
    "    if sample_limit:\n",
    "        data = data[:sample_limit]\n",
    "        logger.info(f\"Sampling {len(data)} items for debugging.\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Leakage Proof Split\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, test_idx = next(gss.split(df, groups=df['product_name']))\n",
    "    \n",
    "    train_df = df.iloc[train_idx]\n",
    "    test_df = df.iloc[test_idx]\n",
    "    \n",
    "    # Overlap Check\n",
    "    overlap = set(train_df.product_name) & set(test_df.product_name)\n",
    "    if overlap:\n",
    "        logger.warning(f\"Leakage detected! {len(overlap)} products overlap.\")\n",
    "    else:\n",
    "        logger.info(\"No Leakage detected.\")\n",
    "        \n",
    "    return DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df[['text', 'label']], preserve_index=False),\n",
    "        \"test\": Dataset.from_pandas(test_df[['text', 'label']], preserve_index=False)\n",
    "    })\n",
    "\n",
    "    import unicodedata\n",
    "    \n",
    "    def find_data_dir():\n",
    "        if is_colab:\n",
    "            return Path(\"/content/data\")\n",
    "            \n",
    "        # Candidates for data root\n",
    "        candidates = [Path(\"./data\"), Path(\"../data\"), Path(\"../../data\")]\n",
    "        \n",
    "        for cand in candidates:\n",
    "            if cand.exists():\n",
    "                logger.info(f\"Found data root at: {cand.resolve()}\")\n",
    "                # Try to find specific fashion folder handling NFD/NFC\n",
    "                # Walk through directory to find '쇼핑몰' and '01. 패션' fuzzily\n",
    "                try:\n",
    "                    shopping_dir = next(cand.glob(\"*쇼핑몰*\"))\n",
    "                    \n",
    "                    # Robust find for '01. 패션' with NFC normalization\n",
    "                    fashion_dir = None\n",
    "                    for p in shopping_dir.glob(\"*\"):\n",
    "                        if \"패션\" in unicodedata.normalize('NFC', p.name):\n",
    "                            fashion_dir = p\n",
    "                            break\n",
    "                    \n",
    "                    if fashion_dir:\n",
    "                        return fashion_dir\n",
    "                    else:\n",
    "                        raise StopIteration\n",
    "                except StopIteration:\n",
    "                    logger.warning(f\"Could not find specific Fashion folder in {cand}\")\n",
    "                    continue\n",
    "                    \n",
    "        return None\n",
    "\n",
    "    DATA_DIR = find_data_dir()\n",
    "    \n",
    "    if DATA_DIR is None:\n",
    "        raise FileNotFoundError(\"Could not locate 'data/쇼핑몰/01. 패션' directory. Please check file structure.\")\n",
    "        \n",
    "    logger.info(f\"Target Data Directory: {DATA_DIR}\")\n",
    "\n",
    "    SAMPLE_LIMIT = 2000 if is_mac_local else None # Full training on Colab\n",
    "\n",
    "    # Load\n",
    "    # Removed try-except to ensure errors are visible and execution stops if data fails\n",
    "    logger.info(f\"Loading data from {DATA_DIR}...\")\n",
    "    dataset = load_and_split_data(DATA_DIR, sample_limit=SAMPLE_LIMIT)\n",
    "    print(dataset)\n",
    "    \n",
    "    if len(dataset['train']) == 0:\n",
    "        raise ValueError(\"Dataset is empty! Check data path and JSON structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 전처리 (Preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"klue/roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=128, padding=False)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1_score[\"f1\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=3)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/full_ft\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3 if not is_mac_local else 1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    fp16=torch.cuda.is_available(), # Use Mixed Precision on CUDA\n",
    "    report_to=\"none\",\n",
    "    gradient_accumulation_steps=2,\n",
    ")\n",
    "\n",
    "trainer_ft = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "if is_mac_local:\n",
    "    print(\"!!! Local Debugging Mode: Skipping Full Training !!!\")\n",
    "    # trainer_ft.train() # Uncomment to debug run\n",
    "else:\n",
    "    trainer_ft.train()\n",
    "    trainer_ft.save_model(\"./saved_models/full_ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PEFT (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset to base model for Fairness\n",
    "model_lora = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=3)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8, # Rank\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"] # Target Attention Layers\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model_lora, lora_config)\n",
    "model_lora.print_trainable_parameters()\n",
    "\n",
    "training_args_lora = TrainingArguments(\n",
    "    output_dir=\"./results/lora\",\n",
    "    learning_rate=1e-4, # Higher LR for LoRA\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3 if not is_mac_local else 1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_lora = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args_lora,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "if is_mac_local:\n",
    "    print(\"!!! Local Debugging Mode: Skipping LoRA Training !!!\")\n",
    "else:\n",
    "    trainer_lora.train()\n",
    "    trainer_lora.save_model(\"./saved_models/lora_ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 비교 분석 및 결과 Summary\n",
    "- Full FT와 LoRA의 F1 Score, Training Time, Model Size를 비교합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

with open("notebooks/ShoppingMall_Sentiment_Analysis.ipynb", "w", encoding='utf-8') as f:
    json.dump(notebook_content, f, indent=1, ensure_ascii=False)

print("Notebook generated: notebooks/ShoppingMall_Sentiment_Analysis.ipynb")
